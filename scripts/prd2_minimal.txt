### Day 4: State Management, Loop Execution & Basic Reflection

- **T4.1:** Verify State Propagation: Ensure data (performance, decision, results) flows correctly between nodes via the `KFMAgentState` object within the graph. (Testing: Requires Debugging/Tracing)
- **T4.2:** Compile LangGraph App: Call `workflow.compile()` to create the runnable application. (Testing: Requires E2E Tests)
- **T4.3:** Implement Reflection Node Structure: Create `reflection_node` function. Add initial logic to check if `kfm_decision` exists in state and `error` is None. (Testing: Requires E2E Tests)
- **T4.4:** Implement `call_llm_for_reflection` function shell: Include prompt formatting using state data, but initially return a mock string instead of making a real API call. (Testing: Requires Unit Test for prompt formatting)
- **T4.5:** Integrate Reflection Call (Mocked): Call the mocked `call_llm_for_reflection` from `reflection_node` and update the state. (Testing: Requires E2E Tests)
- **T4.6:** **Integration Testing (Compiled Graph):** Invoke the compiled `kfm_app` with an initial input state. Trace the execution flow through all nodes (including mocked reflection). Verify the final state contains expected values. (Verification: Debugging, Log Review, Final State Inspection)
- **T4.7:** **Debugging:** Utilize Cursor AI features and print/logging statements to step through the graph execution and diagnose state propagation issues. (Verification: Manual Debugging Session)
- **T4.8:** **Error Handling:** Implement basic `try...except` blocks within the `execute_action_node` to catch potential errors during component execution. Update the `error` field in the state if an exception occurs. (Verification: Code Review, Error Injection Test)
- **T4.9:** **Logging:** Refine logging to clearly show entry/exit of each node and key state values being passed/modified. (Verification: Log Review)
### Day 5: Live Reflection & End-to-End Testing

- **T5.1:** Define Reflection Prompt Requirements: Finalize the exact text and structure of the prompt sent to the LLM, ensuring it clearly states the KFM action and outcome. (Artifact: Prompt String in Code)
- **T5.2:** Implement Live LLM Call: Replace the mock response in `call_llm_for_reflection` with the actual `google.generativeai` API call using the configured API key. (Testing: Requires Live API Test)
- **T5.3:** Add Error Handling for API Call: Include `try...except` around the LLM API call to handle potential network or API errors gracefully, logging the error. (Testing: Requires Mocking API Errors)
- **T5.4:** **API Key Management:** Verify API key is loaded securely from environment variables and not hardcoded. (Verification: Code Review, Security Check)
- **T5.5:** **End-to-End Testing (Live Reflection):** Run the full `kfm_app` multiple times with different initial inputs designed to trigger various KFM rule conditions ('Kill', 'Marry', No Action). (Verification: Test Execution & Log Review)
- **T5.6:** **Verification Steps (E2E):** For each test run: (a) Verify the correct KFM decision is logged. (b) Verify the Component Registry state reflects the decision. (c) Verify the correct component function is logged as being executed. (d) Verify the reflection node is triggered appropriately. (e) Verify the LLM reflection output is logged (or skipped correctly). (f) Verify the final state is as expected. (Verification: Manual Checklist against Logs/State)
- **T5.7:** **Requirements Validation (MVP-REQ Check):** Systematically check if each MVP requirement (MVP-REQ-001 to MVP-REQ-012) is met by the completed implementation and verified by tests. (Artifact: Requirements Traceability Matrix/Checklist) 