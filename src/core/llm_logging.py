import logging
import json
from datetime import datetime
from typing import Any, Dict, List, Optional, Union
from uuid import UUID

from langchain_core.callbacks.base import BaseCallbackHandler
from langchain_core.outputs import LLMResult
from langchain_core.messages import BaseMessage


class JsonFormatter(logging.Formatter):
    """
    Custom formatter to output log records as JSON.
    Standard log record attributes are included, and any additional
    attributes passed in `extra={'props': ...}` are merged.
    """
    def format(self, record: logging.LogRecord) -> str:
        log_entry: Dict[str, Any] = {
            "timestamp": datetime.utcfromtimestamp(record.created).isoformat() + "Z",
            "log_level": record.levelname,
            "message": record.getMessage(), # Ensures args are formatted into message
            "module": record.module,
            "function": record.funcName,
            "line_no": record.lineno,
        }

        # Merge custom properties passed via extra={'props': ...}
        if hasattr(record, 'props') and isinstance(record.props, dict):
            log_entry.update(record.props)
        
        # Ensure event_type is present if props were added, default if not.
        if 'props' in record.__dict__ and 'event_type' not in log_entry:
            log_entry['event_type'] = 'generic_log'


        if record.exc_info:
            log_entry['exception'] = self.formatException(record.exc_info)
        if record.stack_info:
            log_entry['stack_info'] = self.formatStack(record.stack_info)

        return json.dumps(log_entry)


class KfmPlannerCallbackHandler(BaseCallbackHandler):
    """
    Custom LangChain Callback Handler for KFMPlannerLlm.
    Logs key events during LLM interactions and chain/tool executions
    in a structured JSON format.
    """

    def __init__(self, logger: logging.Logger, run_id_for_all_logs: Optional[UUID] = None):
        """
        Initializes the callback handler.

        Args:
            logger: The logger instance to use for outputting logs.
            run_id_for_all_logs: If provided, this run_id will be added to all log entries
                                 generated by this handler instance. Useful for grouping
                                 all logs related to a single overarching execution.
        """
        super().__init__()
        self.logger = logger
        self.run_id_for_all_logs = run_id_for_all_logs

    def _log(self, level: int, message: str, event_type: str, **kwargs: Any) -> None:
        """Helper method to structure and send log messages."""
        props = {"event_type": event_type}
        props.update(kwargs)
        if self.run_id_for_all_logs:
            props["execution_run_id"] = str(self.run_id_for_all_logs)
        
        # Ensure run_id from LangChain is also captured if different or more specific
        # Check if 'run_id' is in kwargs before trying to access it
        lc_run_id_val = kwargs.get('run_id')
        if isinstance(lc_run_id_val, UUID):
            props['lc_run_id'] = str(lc_run_id_val)
        elif lc_run_id_val is not None: # if it's already a string or other type
             props['lc_run_id'] = lc_run_id_val

        # Remove LangChain's run_id from the top-level props if it was part of kwargs,
        # as we've explicitly mapped it to props['lc_run_id']
        if 'run_id' in props: # This refers to run_id passed directly in kwargs to _log
            props.pop('run_id', None)

        self.logger.log(level, message, extra={'props': props})

    def on_llm_start(
        self,
        serialized: Dict[str, Any],
        prompts: List[str],
        *,
        run_id: UUID,
        parent_run_id: Optional[UUID] = None,
        tags: Optional[List[str]] = None,
        metadata: Optional[Dict[str, Any]] = None,
        **kwargs: Any,
    ) -> None:
        details = {
            "lc_run_id": str(run_id), # Langchain's run_id for this specific event
            "parent_lc_run_id": str(parent_run_id) if parent_run_id else None,
            "serialized_llm": serialized,
            "prompts": prompts,
            "tags": tags or [],
            "metadata": metadata or {},
            "additional_kwargs": kwargs,
        }
        self._log(logging.INFO, "LLM call started.", "llm_start", **details)

    def on_llm_new_token(
        self,
        token: str,
        *,
        chunk = None, # Adjusted based on common BaseCallbackHandler signatures
        run_id: UUID,
        parent_run_id: Optional[UUID] = None,
        tags: Optional[List[str]] = None,
        **kwargs: Any,
    ) -> None:
        details = {
            "lc_run_id": str(run_id),
            "parent_lc_run_id": str(parent_run_id) if parent_run_id else None,
            "token_start": token[:50], # Log start of token for brevity
            "tags": tags or [],
        }
        self._log(logging.DEBUG, f"LLM new token received.", "llm_new_token", **details)

    def on_llm_end(
        self,
        response: LLMResult,
        *,
        run_id: UUID,
        parent_run_id: Optional[UUID] = None,
        tags: Optional[List[str]] = None,
        **kwargs: Any,
    ) -> None:
        details = {
            "lc_run_id": str(run_id),
            "parent_lc_run_id": str(parent_run_id) if parent_run_id else None,
            "response": response.dict(), # Convert LLMResult to dict for serialization
            "tags": tags or [],
            "additional_kwargs": kwargs,
        }
        self._log(logging.INFO, "LLM call ended.", "llm_end", **details)

    def on_llm_error(
        self,
        error: Union[Exception, KeyboardInterrupt],
        *,
        run_id: UUID,
        parent_run_id: Optional[UUID] = None,
        tags: Optional[List[str]] = None,
        **kwargs: Any,
    ) -> None:
        details = {
            "lc_run_id": str(run_id),
            "parent_lc_run_id": str(parent_run_id) if parent_run_id else None,
            "error_type": error.__class__.__name__,
            "error_message": str(error),
            "tags": tags or [],
            "additional_kwargs": kwargs,
        }
        self._log(logging.ERROR, "LLM call failed.", "llm_error", **details)

    def on_chain_start(
        self,
        serialized: Dict[str, Any],
        inputs: Dict[str, Any],
        *,
        run_id: UUID,
        parent_run_id: Optional[UUID] = None,
        tags: Optional[List[str]] = None,
        metadata: Optional[Dict[str, Any]] = None,
        **kwargs: Any,
    ) -> None:
        details = {
            "lc_run_id": str(run_id),
            "parent_lc_run_id": str(parent_run_id) if parent_run_id else None,
            "serialized_chain": serialized,
            "inputs": inputs, # Be mindful of PII/large data
            "tags": tags or [],
            "metadata": metadata or {},
            "additional_kwargs": kwargs,
        }
        self._log(logging.INFO, f"Chain '{serialized.get('id', ['UnknownChain'])[-1]}' started.", "chain_start", **details) # Use last part of id path as name

    def on_chain_end(
        self,
        outputs: Dict[str, Any],
        *,
        run_id: UUID,
        parent_run_id: Optional[UUID] = None,
        tags: Optional[List[str]] = None,
        **kwargs: Any,
    ) -> None:
        details = {
            "lc_run_id": str(run_id),
            "parent_lc_run_id": str(parent_run_id) if parent_run_id else None,
            "outputs": outputs, # Be mindful of PII/large data
            "tags": tags or [],
            "additional_kwargs": kwargs,
        }
        self._log(logging.INFO, "Chain ended.", "chain_end", **details)

    def on_chain_error(
        self,
        error: Union[Exception, KeyboardInterrupt],
        *,
        run_id: UUID,
        parent_run_id: Optional[UUID] = None,
        tags: Optional[List[str]] = None,
        **kwargs: Any,
    ) -> None:
        details = {
            "lc_run_id": str(run_id),
            "parent_lc_run_id": str(parent_run_id) if parent_run_id else None,
            "error_type": error.__class__.__name__,
            "error_message": str(error),
            "tags": tags or [],
            "additional_kwargs": kwargs,
        }
        self._log(logging.ERROR, "Chain failed.", "chain_error", **details)

    def on_tool_start(
        self,
        serialized: Dict[str, Any],
        input_str: str,
        *,
        run_id: UUID,
        parent_run_id: Optional[UUID] = None,
        tags: Optional[List[str]] = None,
        metadata: Optional[Dict[str, Any]] = None,
        **kwargs: Any,
    ) -> None:
        details = {
            "lc_run_id": str(run_id),
            "parent_lc_run_id": str(parent_run_id) if parent_run_id else None,
            "serialized_tool": serialized,
            "input_str": input_str,
            "tags": tags or [],
            "metadata": metadata or {},
            "additional_kwargs": kwargs,
        }
        self._log(logging.INFO, f"Tool '{serialized.get('name', 'UnknownTool')}' started.", "tool_start", **details)

    def on_tool_end(
        self,
        output: str, # Or Any, depending on tool output type
        *,
        run_id: UUID,
        parent_run_id: Optional[UUID] = None,
        tags: Optional[List[str]] = None,
        **kwargs: Any,
    ) -> None:
        details = {
            "lc_run_id": str(run_id),
            "parent_lc_run_id": str(parent_run_id) if parent_run_id else None,
            "output": output,
            "tags": tags or [],
            "additional_kwargs": kwargs,
        }
        self._log(logging.INFO, "Tool ended.", "tool_end", **details)

    def on_tool_error(
        self,
        error: Union[Exception, KeyboardInterrupt],
        *,
        run_id: UUID,
        parent_run_id: Optional[UUID] = None,
        tags: Optional[List[str]] = None,
        **kwargs: Any,
    ) -> None:
        details = {
            "lc_run_id": str(run_id),
            "parent_lc_run_id": str(parent_run_id) if parent_run_id else None,
            "error_type": error.__class__.__name__,
            "error_message": str(error),
            "tags": tags or [],
            "additional_kwargs": kwargs,
        }
        self._log(logging.ERROR, "Tool failed.", "tool_error", **details)

    def on_agent_action(
        self,
        action: Any, # Type depends on agent: AgentAction, dict, etc.
        *,
        run_id: UUID,
        parent_run_id: Optional[UUID] = None,
        tags: Optional[List[str]] = None,
        **kwargs: Any,
    ) -> None:
        action_details = {}
        if hasattr(action, 'tool') and hasattr(action, 'tool_input') and hasattr(action, 'log'):
            action_details = {
                'tool': action.tool,
                'tool_input': action.tool_input,
                'agent_log_message': action.log
            }
        else:
            action_details = action # Store as is if not standard AgentAction

        details = {
            "lc_run_id": str(run_id),
            "parent_lc_run_id": str(parent_run_id) if parent_run_id else None,
            "action": action_details,
            "tags": tags or [],
            "additional_kwargs": kwargs,
        }
        self._log(logging.INFO, "Agent action taken.", "agent_action", **details)

    def on_agent_finish(
        self,
        finish: Any, # Type depends on agent: AgentFinish, dict, etc.
        *,
        run_id: UUID,
        parent_run_id: Optional[UUID] = None,
        tags: Optional[List[str]] = None,
        **kwargs: Any,
    ) -> None:
        finish_payload = {}
        if hasattr(finish, 'return_values') and hasattr(finish, 'log'):
            finish_payload = {
                'return_values': finish.return_values,
                'agent_log_message': finish.log
            }
        else:
            finish_payload = finish # Store as is
            
        details = {
            "lc_run_id": str(run_id),
            "parent_lc_run_id": str(parent_run_id) if parent_run_id else None,
            "finish_details": finish_payload,
            "tags": tags or [],
            "additional_kwargs": kwargs,
        }
        self._log(logging.INFO, "Agent finished.", "agent_finish", **details)


def get_configured_logger(name: str) -> logging.Logger:
    logger = logging.getLogger(name)
    if not logger.handlers:
        logger.setLevel(logging.DEBUG)
        ch = logging.StreamHandler()
        ch.setLevel(logging.DEBUG)
        formatter = JsonFormatter()
        ch.setFormatter(formatter)
        logger.addHandler(ch)
        # logger.propagate = False # Uncomment if you don't want root logger to also handle these
    return logger

if __name__ == '__main__':
    import uuid
    main_logger = get_configured_logger('kfm_planner_llm.main_example')
    execution_run_id = uuid.uuid4()
    kfm_callback_handler = KfmPlannerCallbackHandler(logger=main_logger, run_id_for_all_logs=execution_run_id)

    test_run_id = uuid.uuid4()
    test_parent_run_id = uuid.uuid4()

    main_logger.info("Starting KFM LLM Process.", extra={"props": {"event_type": "process_start", "process_id": str(execution_run_id)}})

    kfm_callback_handler.on_llm_start(
        serialized={"id": ["llms", "TestLLM"], "name": "TestLLM"}, # id is often a path
        prompts=["What is your name?"],
        run_id=test_run_id,
        parent_run_id=test_parent_run_id,
        tags=["test", "example"],
        metadata={"model_provider": "OpenAI"}
    )

    class MockLLMResult:
        def __init__(self, text_generation: str):
            self.generations = [[MockGeneration(text_generation)]]
            self.llm_output = {"token_usage": {"input_tokens": 10, "output_tokens": 5}, "model_name": "gpt-test"}
        def dict(self):
            return {
                "generations": [[gen.dict() for gen in self.generations[0]]],
                "llm_output": self.llm_output
            }

    class MockGeneration:
        def __init__(self, text: str):
            self.text = text
            self.generation_info = {"finish_reason": "stop"}
        def dict(self):
            return {"text": self.text, "generation_info": self.generation_info}

    mock_response = MockLLMResult("My name is TestLLM.")
    kfm_callback_handler.on_llm_end(response=mock_response, run_id=test_run_id, parent_run_id=test_parent_run_id)

    kfm_callback_handler.on_chain_start(
        serialized={"id": ["chains","TestChain"]}, 
        inputs={"input_key": "input_value"}, 
        run_id=uuid.uuid4(), 
        parent_run_id=test_run_id
    )

    kfm_callback_handler.on_llm_error(
        error=ValueError("Simulated LLM API error"),
        run_id=uuid.uuid4(),
        parent_run_id=test_run_id,
        tags=["error_simulation"]
    )
    
    main_logger.info(
        "Application specific event.", 
        extra={"props": {"event_type": "custom_app_event", "user_id": "user123", "details": {"key": "value"}}}
    )

    try:
        raise RuntimeError("A simulated critical error")
    except RuntimeError:
        main_logger.error(
            "A critical application error occurred.",
            exc_info=True, 
            extra={"props": {"event_type": "app_critical_error", "error_code": 5001}}
        )
    
    print("\nExample logs would be printed above in JSON format.")
    print("If a file handler was added to get_configured_logger, check the specified log file.") 