# KFMPlannerLlm Logging and Error Handling

This document describes the structured logging and error handling mechanisms implemented for the `KFMPlannerLlm` and its integration within the LangGraph workflow.

## Introduction

To ensure traceability, debuggability, and observability of the LLM-based KFM decision process, a standardized logging and error handling system has been implemented. Logs are structured in JSON format for easier machine parsing and analysis, while errors are handled using custom exceptions and propagated consistently through the system state.

## Logging

### Schema

All relevant events are logged in JSON format. Each log record generally follows this base structure:

```json
{
    "timestamp": "YYYY-MM-DDTHH:MM:SS.sssZ", // ISO 8601 UTC timestamp
    "log_level": "INFO",                   // e.g., DEBUG, INFO, WARNING, ERROR, CRITICAL
    "message": "Human-readable log message.", // Core message
    "module": "module_name",               // Python module source (e.g., kfm_planner_llm)
    "function": "function_name",           // Function source (e.g., decide_kfm_action)
    "line_no": 123,                        // Line number in the source file
    // --- Custom Properties Start Here ---
    "event_type": "specific_event_identifier", // Unique identifier for the type of event
    // ... other event-specific key-value pairs (see below) ...
}
```

Custom properties are added via the `extra={'props': {...}}` argument in logger calls. The `JsonFormatter` defined in `src.core.llm_logging` merges these properties into the final JSON output.

### Key Properties & Event Types

*   **`event_type`**: A crucial field identifying the specific event being logged. Common values include:
    *   **LLM Interaction (from `KfmPlannerCallbackHandler`):**
        *   `llm_start`: LLM call initiated. Includes `prompts`, `serialized_llm`.
        *   `llm_new_token`: (DEBUG level) New token received during streaming.
        *   `llm_end`: LLM call completed successfully. Includes `response` (LLMResult as dict).
        *   `llm_error`: LLM call failed. Includes `error_type`, `error_message`.
        *   `chain_start`, `chain_end`, `chain_error`: For LangChain chains.
        *   `tool_start`, `tool_end`, `tool_error`: For LangChain tools.
        *   `agent_action`, `agent_finish`: For LangChain agents.
    *   **KFMPlannerLlm (`decide_kfm_action`):**
        *   `planner_init`: KFMPlannerLlm instance initialized. Includes `llm_class`, `execution_id`.
        *   `decide_kfm_action_start`: Entry into the decision method. Includes `task_name`, `task_requirements_keys`, `components_performance_count`.
        *   `llm_chain_invocation_input`: (DEBUG level) Input dictionary passed to the LLM chain invoke call. Includes `llm_input`.
        *   `decide_kfm_action_success`: Successful decision made. Includes `action`, `component`, `confidence`, `reasoning`.
        *   `decide_kfm_action_input_validation_error` / `_warning`: Input validation failed/warned. Includes `validation_error_type`, `details`.
        *   `decide_kfm_action_json_conversion_error`: Failed to dump input data to JSON. Includes `error_message`.
        *   `decide_kfm_action_parser_error`: Failed to parse LLM output. Includes `error_message`, `error_class`, `llm_output_attempt`.
        *   `decide_kfm_action_invocation_api_error`: API/Network error during LLM call (after retries). Includes `error_message`, `error_class`, `original_exception_type`.
        *   `decide_kfm_action_invocation_unexpected_error`: Other unexpected error during LLM call. Includes `error_message`, `error_class`, `original_exception_type`.
    *   **LangGraph Nodes (`src/langgraph_nodes.py`):**
        *   `kfm_decision_node_start`, `kfm_decision_node_end`: Entry/Exit. Log state info, result.
        *   `kfm_decision_node_planner_result`: Logs the raw result dict from `KFMPlannerLlm`.
        *   `kfm_decision_node_planner_error_reported`: Warning if the planner result contains `error_info`.
        *   `kfm_decision_node_planner_call_exception`: If the call to the planner itself fails.
        *   `should_fallback_check`: Before evaluating fallback condition. Logs inputs to decision.
        *   `fallback_node_start`, `fallback_node_end`: Entry/Exit. Logs trigger reason, decisions.
        *   `fallback_node_rule_based_planner_success`/`_failed`: Outcome of trying the rule-based planner.
*   **`execution_run_id`**: (Optional) A UUID passed during `KFMPlannerLlm` instantiation. It's automatically added by `KfmPlannerCallbackHandler` to all logs it generates, allowing grouping of all logs related to a single "run" of the planner.
*   **`lc_run_id`**: The specific run ID assigned by LangChain to an individual LLM call, chain run, or tool run. Useful for tracing within LangChain's ecosystem (e.g., LangSmith).
*   **`correlation_id`**: (In LangGraph nodes) An ID generated by the tracing system (`src.tracing`) to link logs across different nodes within a single graph execution.

### Configuration

Logging uses Python's standard `logging` module. Configuration (setting levels, handlers, formatters) should be done centrally, typically using `logging.config.dictConfig` at application startup. The system relies on the `JsonFormatter` from `src.core.llm_logging`.

## Error Handling

### Custom Exceptions

A set of custom exceptions are defined in `src.core.kfm_llm_exceptions.py`:

*   `KfmPlannerError`: Base class.
*   `KfmValidationError`: For input validation issues.
*   `KfmJsonConversionError`: For errors converting data to JSON for the LLM.
*   `KfmOutputParsingError`: When LLM output doesn't match the expected `KFMDecision` Pydantic model. Includes `llm_output`.
*   `KfmInvocationError`: For errors during the LLM chain invocation (e.g., API errors, network errors after retries, unexpected errors). Includes `original_exception`.
*   `KfmReasoningError`: (Less common) For explicit reasoning failures reported by the LLM.
*   `KfmConfigurationError`: For setup/configuration issues.

These exceptions store an `error_type` string and an optional `details` dictionary.

### Error Handling in `KFMPlannerLlm`

The `KFMPlannerLlm.decide_kfm_action` method uses `try...except` blocks to catch these custom exceptions as well as standard exceptions like `OutputParserException`, API errors, etc.

When an error is caught:
1.  It's wrapped (if necessary) into one of the custom `KfmPlannerError` types.
2.  A structured ERROR log is generated using the `_handle_error_and_return_kill` helper.
3.  The method returns a standardized dictionary representing a 'kill' decision, which includes an `error_info` field:

    ```python
    {
        'action': 'kill', 
        'component': None, 
        'reasoning': "Kill decision due to error: [Error message]", 
        'confidence': 1.0, 
        'error_info': {
            'error_type': "KfmValidationError", # The specific KfmError type or original class name
            'message': "Detailed error message.",
            'details': { ... } # Optional dictionary from the exception
        }
    }
    ```
4.  If the decision is successful, `error_info` is explicitly set to `None`.

### Error Propagation in LangGraph

*   The `kfm_decision_node` receives the result dictionary from `KFMPlannerLlm`.
*   The `should_fallback` conditional logic checks for the presence of `error_info` in the decision result. The current policy is to **trigger fallback** if `error_info` is present, regardless of confidence, allowing a rule-based system or manual review to handle planner errors.
*   The `fallback_node` logs the original decision (including `error_info`) that triggered the fallback.

## Debugging Guide

1.  **Trace a Full Execution:** Filter logs by a specific `execution_run_id` (if provided during `KFMPlannerLlm` instantiation) to see all related logs from the callback handler. Alternatively, use the `correlation_id` to trace across different LangGraph nodes for a single run.
2.  **Identify Errors:** Filter logs by `log_level: ERROR` or `log_level: WARNING`. Look at the `event_type` and `error_type` fields for specific failure points (e.g., `decide_kfm_action_parser_error`, `llm_error`).
3.  **Examine LLM Calls:** Look for `llm_start` events to see the exact `prompts` sent and `llm_end` to see the `response`. For `llm_error`, check the `error_message`.
4.  **Check Parsing Errors:** If `event_type` is `decide_kfm_action_parser_error`, examine the `details.llm_output_attempt` field in the log to see the raw LLM output that failed parsing.
5.  **Analyze Final State:** If the graph execution completed but resulted in an error, check the `error` field in the final `KFMAgentState`. The `error_info` returned by `decide_kfm_action` (if it was the source of the error) should be contained within this state field. 